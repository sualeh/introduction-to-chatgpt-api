{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sualeh/introduction-to-chatgpt-api/blob/main/chatgpt-api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "> **How to Run This Notebook**\n",
    "\n",
    "To get started, create an Open AI API account, set up billing, and generate and API key at https://platform.openai.com/. If you are running the notebook locally in Visual Studio Code or other IDE, create a file called `.env`, and add a line `OPENAI_API_KEY=<your-openai-api-key>`. This key will be read by the `load_dotenv` library.\n",
    "\n",
    "Otherwise, if you are running in Google Colab, create a secret called `OPENAI_API_KEY` and set it to the value of your OpenAI API key.\n",
    "\n",
    "Run the code below to read the key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq python-dotenv\n",
    "\n",
    "from os import environ as env\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Load key from an environmental variable called \"OPENAI_API_KEY\"\n",
    "# Use python-dotenv https://pypi.org/project/python-dotenv/\n",
    "# And take environment variables from .env\n",
    "load_dotenv()\n",
    "try:\n",
    "  # Attempt to read OPENAI_API_KEY from a Google Colab secret\n",
    "  from google.colab import userdata\n",
    "  env['OPENAI_API_KEY'] = env.get('OPENAI_API_KEY', userdata.get('OPENAI_API_KEY'))\n",
    "except ModuleNotFoundError:\n",
    "  logger.info(\"Not running in Google Colab\")\n",
    "  # No action - rely on the OPENAI_API_KEY environmental variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Retrieval Augmented Generation (RAG) with Vector Databases\n",
    "\n",
    "Let us see how to use the vector database we created earlier as a knowledge base for OpenAI's ChatGPT model. This creates a powerful RAG (Retrieval Augmented Generation) system that can answer questions based on your local documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "%pip install -qq langchain langchain_openai faiss-cpu python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's import the required libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if the API key is available\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    openai_api_key = input(\"Please enter your OpenAI API key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Initialize the OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Set default vector database path\n",
    "VECTOR_DB_PATH = \"./vector_db\"\n",
    "\n",
    "# Load parameter configuration\n",
    "load_dotenv(dotenv_path=\".env.params\")\n",
    "if os.getenv(\"VECTOR_DB_PATH\"):\n",
    "    VECTOR_DB_PATH = os.getenv(\"VECTOR_DB_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vector Database\n",
    "\n",
    "Now let's create a function to load our previously created vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def load_vector_database(db_path=VECTOR_DB_PATH):\n",
    "    \"\"\"\n",
    "    Load a FAISS vector database from the specified path.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the vector database\n",
    "        \n",
    "    Returns:\n",
    "        FAISS: The loaded vector database or None if not found\n",
    "    \"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        logger.error(f\"Error: Vector database not found at {db_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        vector_db = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        logger.info(f\"Vector database successfully loaded from {db_path}\")\n",
    "        return vector_db\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading vector database: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Chat Model\n",
    "\n",
    "We need to load up the previously created vector database that contains embedding, set up the OpenAI ChatGPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Load the vector database\n",
    "vector_db = load_vector_database()\n",
    "\n",
    "# Set up the chat model, using langchain\n",
    "chat_model = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "# Create the RAG prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are a helpful assistant that provides accurate information based on the given context.\n",
    "    If you don't know the answer based on the context, just say that you don't know.\n",
    "    Don't try to make up an answer.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\")\n",
    "\n",
    "# Create a retriever from the vector database\n",
    "k = 3\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# Format the retrieved documents into a single context string\n",
    "# Also include source numbers for citation\n",
    "def format_docs(docs):\n",
    "    # DEBUG: Print the retrieved documents\n",
    "    logger.info(f\"Retrieved {len(docs)} documents:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        source_info = f\"Source [{i+1}]\"\n",
    "        print(doc.metadata) \n",
    "        if 'source' in doc.metadata:\n",
    "            source_info = source_info + f\": {doc.metadata['source']}\"\n",
    "        page_content = doc.page_content.replace(\"\\n\", \"\")\n",
    "        logger.info(f\"{source_info}\\n\\t{page_content[:50]} ... {page_content[-50:]}\")\n",
    "\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What are the main topics covered in the most commonly referenced documents?\"\n",
    "print(f\"\\nQuestion: {question}\\n\\n\")\n",
    "answer = rag_chain.invoke(question)\n",
    "print(f\"\\n\\nAnswer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
