{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sualeh/introduction-to-chatgpt-api/blob/main/chatgpt-rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAaoVeg0yZ7L"
      },
      "source": [
        "----------\n",
        "\n",
        "> **How to Run This Notebook**\n",
        "\n",
        "To get started, create an Open AI API account, set up billing, and generate and API key at https://platform.openai.com/. If you are running the notebook locally in Visual Studio Code or other IDE, create a file called `.env`, and add a line `OPENAI_API_KEY=<your-openai-api-key>`. This key will be read by the `load_dotenv` library.\n",
        "\n",
        "Otherwise, if you are running in Google Colab, create a secret called `OPENAI_API_KEY` and set it to the value of your OpenAI API key.\n",
        "\n",
        "Run the code below to read the key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1OiJxD4yZ7M"
      },
      "outputs": [],
      "source": [
        "%pip install -qq python-dotenv\n",
        "\n",
        "from os import environ as env\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "# Load key from an environmental variable called \"OPENAI_API_KEY\"\n",
        "# Use python-dotenv https://pypi.org/project/python-dotenv/\n",
        "# And take environment variables from .env\n",
        "load_dotenv()\n",
        "try:\n",
        "  # Attempt to read OPENAI_API_KEY from a Google Colab secret\n",
        "  from google.colab import userdata\n",
        "  env['OPENAI_API_KEY'] = env.get('OPENAI_API_KEY', userdata.get('OPENAI_API_KEY'))\n",
        "except ModuleNotFoundError:\n",
        "  logger.info(\"Not running in Google Colab\")\n",
        "  # No action - rely on the OPENAI_API_KEY environmental variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN0hAxFVyZ7N"
      },
      "source": [
        "\n",
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x0CJrN-yZ7N"
      },
      "source": [
        "# Embedding and Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC5ydcl3yZ7N"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azZaEz0TyZ7N"
      },
      "outputs": [],
      "source": [
        "%pip install -qq openai\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# Create Open AI client to use ChatGPT\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQX9HQMGyZ7N"
      },
      "source": [
        "## Embedding\n",
        "\n",
        "Convert your text into an embedding vector. An embedding vector is a vector of floating point values. Embeddings translate human language into a mathematical form that AI models can understand. When text is converted to these numerical vectors, documents or pieces of text with similar meanings end up close to each other in the vector space, even if they use different words to express the same ideas. This enables machines to understand semantic relationships beyond simple keyword matching.\n",
        "\n",
        "By default, the length of the embedding vector will be 1,536 for \"text-embedding-3-small\" or 3,072 for \"text-embedding-3-large\". You can reduce the dimensions of the embedding by passing in the dimensions parameter without the embedding losing its concept-representing properties. (From [What are Embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veilx8QXyZ7N"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
        "\n",
        "print(get_embedding(\"Hello, World!\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR-DSxb-yZ7N"
      },
      "source": [
        "A simple cosine function can find similarity between vectors. Cosine similarity measures the cosine of the angle between two vectors. It ranges from -1 (completely opposite) to 1 (exactly the same), with 0 indicating orthogonality (no relationship). When comparing embedding vectors:\n",
        "\n",
        "* A value close to 1 means the texts have very similar semantic meaning\n",
        "* A value close to 0 means the texts are unrelated\n",
        "* A value close to -1 is rare with embeddings and would indicate opposite meanings\n",
        "\n",
        "The function below calculates this similarity measure, which we will use to determine which content is most relevant to a query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cetj3kXnyZ7O"
      },
      "outputs": [],
      "source": [
        "%pip install -qq scipy\n",
        "\n",
        "from scipy import spatial\n",
        "\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    return 1 - spatial.distance.cosine(vector1, vector2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0LFHkS-yZ7O"
      },
      "source": [
        "## Find Similarity\n",
        "\n",
        "Find the embedding vectors between the user query and blocks of content, and then find the most similar content. If there is a question being asked, this is where the answer is likely to be.\n",
        "\n",
        "Here is what happens in the code below:\n",
        "\n",
        "1. We start with a collection of quotes (which simulates a document database)\n",
        "2. The user asks a question (the prompt or query)\n",
        "3. We convert both the query and all quotes into embedding vectors\n",
        "4. We calculate the similarity between the query vector and each quote vector\n",
        "5. We rank the quotes by similarity score\n",
        "6. The quote with the highest similarity score is likely to contain the information needed to answer the query\n",
        "\n",
        "In real-world applications, this same process might search through thousands or millions of documents to find the most relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6d7P1GsyZ7O"
      },
      "outputs": [],
      "source": [
        "quotes = [\n",
        "    '\"Stop worrying about the potholes in the road and enjoy the journey\" - Babs Hoffman.',\n",
        "    '\"A journey of a thousand miles begins with a single step\" - Chinese proverb',\n",
        "    '\"One small step for man, one giant leap for mankind.\" - Neil Armstrong'\n",
        "]\n",
        "user_query = \"What's a famous saying about a long trip?\"\n",
        "\n",
        "# Encode the query\n",
        "query_vector = get_embedding(user_query)\n",
        "\n",
        "# Calculate cosine similarities between the user query and the quotes\n",
        "scores = []\n",
        "for i, quote in enumerate(quotes):\n",
        "    vector = get_embedding(quote)\n",
        "    similarity = cosine_similarity(query_vector, vector)\n",
        "    print(f'{quote[:50]}â€¦ - {similarity}')\n",
        "    scores.append((similarity, quote))\n",
        "\n",
        "# Find most similar text by sorting to find the highest similarity score\n",
        "scores.sort(reverse=True)\n",
        "most_similar_text = scores[0][1]\n",
        "\n",
        "print()\n",
        "print(f'The text most similar to the query:\\n\\t{user_query}\\nis:\\n\\t{most_similar_text}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0dATT1ZyZ7O"
      },
      "source": [
        "Embeddings can saved for future use. For large datasets, use a vector database. Vector databases can quickly find the vectors in the database that are most similar to a given query vector. Vector databases are specialized for storing and searching embedding vectors efficiently. They use algorithms like Approximate Nearest Neighbor (ANN) to quickly find similar vectors without having to compare against every vector in the database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gv7A_-MyZ7O"
      },
      "source": [
        "## Retrieval Augmented Generation (RAG)\n",
        "\n",
        "What we are doing here is a simplified version of Retrieval Augmented Generation (RAG). We have already found the most relevant text (the quote) to our query using embedding similarity. We include this relevant text in the system message to give ChatGPT context, and also send the user's query as the user message. ChatGPT can now provide a more accurate, grounded response because it has the specific information needed.\n",
        "\n",
        "This approach has several benefits:\n",
        "* It reduces hallucinations since the model has specific facts to work with\n",
        "* It makes responses more precise and relevant\n",
        "* It can save tokens/ costs as you are focusing the model on relevant information\n",
        "* It allows the model to work with up-to-date or domain-specific information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zZR6aSwyZ7O"
      },
      "outputs": [],
      "source": [
        "user_prompt = \"What's a famous saying about a long trip? How many steps start a journey? Answer with a numeric value.\"\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": most_similar_text},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ],\n",
        ")\n",
        "\n",
        "reply = chat_completion.choices[0].message.content\n",
        "print(reply)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
