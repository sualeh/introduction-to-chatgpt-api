{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sualeh/introduction-to-chatgpt-api/blob/main/chatgpt-api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l50CdqTioXqF"
      },
      "source": [
        "----------\n",
        "\n",
        "> **How to Run This Notebook**\n",
        "\n",
        "To get started, create an Open AI API account, set up billing, and generate and API key at https://platform.openai.com/. If you are running the notebook locally in Visual Studio Code or other IDE, create a file called `.env`, and add a line `OPENAI_API_KEY=<your-openai-api-key>`. This key will be read by the `load_dotenv` library.\n",
        "\n",
        "Otherwise, if you are running in Google Colab, create a secret called `OPENAI_API_KEY` and set it to the value of your OpenAI API key.\n",
        "\n",
        "Run the code below to read the key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wRN0CzlrrvQ"
      },
      "outputs": [],
      "source": [
        "%pip install -qq python-dotenv\n",
        "\n",
        "from os import environ as env\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "# Load key from an environmental variable called \"OPENAI_API_KEY\"\n",
        "# Use python-dotenv https://pypi.org/project/python-dotenv/\n",
        "# And take environment variables from .env\n",
        "load_dotenv()\n",
        "try:\n",
        "  # Attempt to read OPENAI_API_KEY from a Google Colab secret\n",
        "  from google.colab import userdata\n",
        "  env['OPENAI_API_KEY'] = env.get('OPENAI_API_KEY', userdata.get('OPENAI_API_KEY'))\n",
        "except ModuleNotFoundError:\n",
        "  logger.info(\"Not running in Google Colab\")\n",
        "  # No action - rely on the OPENAI_API_KEY environmental variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qae9yrErzUm"
      },
      "source": [
        "\n",
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P2Hkt3joXqG"
      },
      "source": [
        "# A Short Introduction to the ChatGPT API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nttO5ggIoXqH"
      },
      "source": [
        "The ChatGPT API is relatively easy to use. It has object bindings in most major languages, so you can use your language of choice.\n",
        "\n",
        "The API allows you to integrate OpenAI's powerful language models like GPT-4 directly into your applications. Unlike using the ChatGPT web interface, the API gives you programmatic access and more control over the model's behavior, allowing you to build custom AI-powered solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbNHzUoeoXqH"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "To use the ChatGPT API, we first need to install the OpenAI Python library and create a client. The client handles authentication and communication with OpenAI's servers, allowing us to make requests to the various models available through the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leu5c9BWoXqH"
      },
      "outputs": [],
      "source": [
        "%pip install -qq openai\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# Create Open AI client to use ChatGPT\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gio5Z92toXqH"
      },
      "source": [
        "## Making a Request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUh5FCyQoXqI"
      },
      "source": [
        "Start writing your first request - you need to specify the model and provide the user prompt.\n",
        "\n",
        "The \"user\" role represents the user's input or message within the conversation. The prompt can include questions, commands, or any other content the user wants to communicate.\n",
        "\n",
        "When making a request to the ChatGPT API, you need to include the following components:\n",
        "\n",
        "1. **Model**: Choose which LLM model to use (e.g., \"gpt-4o\")\n",
        "2. **Messages**: A list of message objects that form the conversation history\n",
        "3. **Optional parameters**: Such as temperature (controls randomness), max_tokens (limits response length), etc.\n",
        "\n",
        "Each message in the messages list has a \"role\" and \"content\". The role can be \"system\", \"user\", or \"assistant\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhRZ_jKEoXqI"
      },
      "outputs": [],
      "source": [
        "prompt = \"When was the first moon landing?\"\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    model=\"gpt-4o\",\n",
        ")\n",
        "\n",
        "print(chat_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99ZF6F_toXqI"
      },
      "source": [
        "The completion has the response content, as well as usage information. Responses are messages from the \"assistant\" role. What we really want to extract, though, is the actual message content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySfQZSZfoXqI"
      },
      "outputs": [],
      "source": [
        "reply = chat_completion.choices[0].message.content\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a06E47xoXqI"
      },
      "source": [
        "## Using the \"system\" Role"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_le2j2REoXqI"
      },
      "source": [
        "The \"system\" role is used to provide high-level instructions or setting the behavior of the assistant. It guides the conversation or specifies the desired outcome. This information is typically provided at the beginning of the conversation.\n",
        "\n",
        "System messages are powerful because they allow you to shape the assistant's behavior without explicitly including these instructions in every user prompt. Some examples of effective system messages include:\n",
        "\n",
        "- \"You are an expert in physics with a PhD. Explain concepts in simple terms.\"\n",
        "- \"Respond with just a single word or short phrase.\"\n",
        "- \"Always include code examples in Python when explaining programming concepts.\"\n",
        "- \"You are a helpful assistant that speaks like Shakespeare.\"\n",
        "\n",
        "System messages help establish the \"personality\" and behavior patterns of the AI assistant throughout the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aztd1x8UoXqI"
      },
      "outputs": [],
      "source": [
        "prompt = \"When was the first moon landing?\"\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Always reply in French.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ],\n",
        ")\n",
        "\n",
        "reply = chat_completion.choices[0].message.content\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-hLPzNmoXqJ"
      },
      "source": [
        "ChatGPT is most useful when it has context from the conversation. You will need to maintain chat history yourself, by keeping the \"system\" and \"user\" messages as well as the \"assistant\" reponses in a list, and passing this list into every API call."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG74OZ2roXqJ"
      },
      "source": [
        "## Maintaining Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rniItILoXqJ"
      },
      "source": [
        "ChatGPT is a REST API, and does not maintain sessions. However, the client can maintain its own chat history, and present that on an API call for additional context on the conversation. To be effective, you would need to maintain both the user questions and the ChatGPT responses. In its simplest form, it is a list of chat messages. By maintaining and passing the conversation history in each request, you can create the illusion of a continuous conversation with the AI, even though each request is technically separate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzPmssuAoXqJ"
      },
      "outputs": [],
      "source": [
        "prompt = \"What is the moon's circumference in km?\"\n",
        "\n",
        "message_history = []\n",
        "message_history.append(\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=message_history\n",
        ")\n",
        "\n",
        "# Print the response:\n",
        "reply_content = chat_completion.choices[0].message.content\n",
        "print(reply_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tQsAND2oXqJ"
      },
      "source": [
        "Let us store the response in the history as well, so we can make a context-sensitive prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cmknrhFoXqJ"
      },
      "outputs": [],
      "source": [
        "message_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWTtpUUpoXqJ"
      },
      "source": [
        "Now, ask an ambiguous question, which is answered in context.\n",
        "\n",
        "Notice how we're asking \"How far is it?\" - without context, this question is meaningless. However, since we've included the previous messages in our history, the AI understands we're referring to the moon's circumference mentioned in the previous exchange.\n",
        "\n",
        "This demonstrates why maintaining conversation history is crucial for creating natural, flowing interactions with AI assistants. Without this context, you would need to restate your entire question with all relevant details each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOS0XV3yoXqJ"
      },
      "outputs": [],
      "source": [
        "prompt = \"How far is it?\"\n",
        "\n",
        "message_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=message_history\n",
        ")\n",
        "\n",
        "reply_content = chat_completion.choices[0].message.content\n",
        "print(reply_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b50L7nMoXqJ"
      },
      "source": [
        "## References\n",
        "\n",
        "- Based on [Sentdex/ChatGPT-API-Basics](https://github.com/Sentdex/ChatGPT-API-Basics)\n",
        "- See [ChatGPT API in Python by sentdex](https://www.youtube.com/watch?v=c-g6epk3fFE)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
