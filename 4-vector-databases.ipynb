{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sualeh/introduction-to-chatgpt-api/blob/main/local-vector-database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAaoVeg0yZ7L"
      },
      "source": [
        "----------\n",
        "\n",
        "> **How to Run This Notebook**\n",
        "\n",
        "To get started, create an Open AI API account, set up billing, and generate and API key at https://platform.openai.com/. If you are running the notebook locally in Visual Studio Code or other IDE, create a file called `.env`, and add a line `OPENAI_API_KEY=<your-openai-api-key>`. This key will be read by the `load_dotenv` library.\n",
        "\n",
        "Otherwise, if you are running in Google Colab, create a secret called `OPENAI_API_KEY` and set it to the value of your OpenAI API key.\n",
        "\n",
        "Run the code below to read the key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1OiJxD4yZ7M"
      },
      "outputs": [],
      "source": [
        "%pip install -qq python-dotenv\n",
        "\n",
        "from os import environ as env\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "# Load key from an environmental variable called \"OPENAI_API_KEY\"\n",
        "# Use python-dotenv https://pypi.org/project/python-dotenv/\n",
        "# And take environment variables from .env\n",
        "load_dotenv()\n",
        "try:\n",
        "  # Attempt to read OPENAI_API_KEY from a Google Colab secret\n",
        "  from google.colab import userdata\n",
        "  env['OPENAI_API_KEY'] = env.get('OPENAI_API_KEY', userdata.get('OPENAI_API_KEY'))\n",
        "except ModuleNotFoundError:\n",
        "  logger.info(\"Not running in Google Colab\")\n",
        "  # No action - rely on the OPENAI_API_KEY environmental variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a technique that enhances Large Language Models by combining two key components:\n",
        "\n",
        "1. **Retrieval**: Finding relevant information from a knowledge base\n",
        "2. **Generation**: Using that information to create accurate, contextual responses\n",
        "\n",
        "RAG helps overcome LLMs' limitations by providing external, up-to-date knowledge without having to retrain the model. In this notebook, we'll build a complete RAG pipeline step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vector Databases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Files\n",
        "\n",
        "The first step in any RAG system is to ingest the knowledge that will later be retrieved. We need to:\n",
        "\n",
        "- Import external knowledge (PDFs, text files, web pages, etc.)\n",
        "- Convert this unstructured data into a structured format (\"langchain\" `Document` objects)\n",
        "- Preserve metadata like page numbers or source information for later citation\n",
        "\n",
        "Document loaders like `PyPDFLoader` from the \"langchain\" library handle the complex task of parsing different file formats and converting them into a standardized `Document` structure that the rest of our RAG pipeline can process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qq langchain langchain-community pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.schema import Document\n",
        "\n",
        "file_path = \"./example.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "loaded_documents: list[Document] = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the loaded document information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_document_chunks(\n",
        "    documents: list[Document], \n",
        "    limit: int = 3,\n",
        "    context: int = 100,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Print preview of document chunks with their metadata.\n",
        "    \n",
        "    Args:\n",
        "        documents: List of Document objects to preview.\n",
        "        limit: Maximum number of chunks to display.\n",
        "    \"\"\"\n",
        "    print(f\"Printing {len(documents)} document chunk(s) with metadata\")\n",
        "    print()\n",
        "    for index, chunk in enumerate(documents):\n",
        "        if index > limit:\n",
        "            break\n",
        "        print(f\"------ CHUNK {index+1} -------------------------------------------------\")\n",
        "        print(chunk.metadata)\n",
        "        print()\n",
        "        print(chunk.page_content[:context])\n",
        "        print(\"... (skipping content) ...\")\n",
        "        print(chunk.page_content[-context:])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_document_chunks(loaded_documents, limit=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Splitting\n",
        "\n",
        "Next, we'll split the documents into smaller chunks for better embedding and retrieval. Large documents need to be broken down into smaller pieces for several reasons:\n",
        "\n",
        "- **Embedding Limitations**: Most embedding models have token limits (e.g., 8,192 tokens for \"text-embedding-ada-002\")\n",
        "- **Retrieval Precision**: Smaller chunks allow for more precise retrieval of relevant information\n",
        "- **Context Windows**: LLMs have limited context windows - retrieving entire documents would waste tokens\n",
        "- **Semantic Focus**: Each chunk should ideally contain coherent, focused information on a specific topic\n",
        "\n",
        "The \"langchain\" `RecursiveCharacterTextSplitter` is intelligent about how it splits text, trying to preserve natural boundaries like paragraphs while respecting maximum chunk sizes. The `chunk_overlap` parameter creates some redundancy between chunks to preserve context that might be split across chunk boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(loaded_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at the chunks of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_document_chunks(chunks, limit=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Vector Database\n",
        "\n",
        "At the core of modern RAG systems are vector embeddings and vector databases:\n",
        "\n",
        "**Text Embeddings** are mathematical representations of text as high-dimensional vectors (e.g., 1,536 dimensions for OpenAI embeddings). These vectors capture semantic relationships - similar meanings cluster together in vector space. Embedding vectors are stored in a **Vector Database**, which is specialized storage that allows for efficient similarity searches in high-dimensional space.\n",
        "\n",
        "The embedding process transforms our text chunks into vectors that preserve their semantic meaning. These vectors are then stored in a vector database, which provides efficient nearest-neighbor search capabilities critical for the retrieval component of RAG. We use **FAISS**, a vector database library developed by Facebook AI Research for efficient similarity search and clustering of dense vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qq faiss-cpu langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create vector database to store the embeddings and perform similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# OpenAI embeddings model\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "\n",
        "# Create a FAISS vector store from the document chunks\n",
        "vector_db = FAISS.from_documents(chunks, embeddings_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Retrieval Process\n",
        "\n",
        "Retrieval is where the 'R' in RAG comes into play. First, the user's question or prompt is converted to the same vector space as our documents. This is done by embedding the query using the same embedding model we used for our documents. A **Similarity Search** in the vector database finds document chunks whose embeddings are closest to the query embedding. These documents are ranked by similarity score (cosine similarity or other distance metrics). We select the k most relevant chunks to provide as context (top-k selection).\n",
        "\n",
        "This similarity-based retrieval is far more powerful than simple keyword matching because it captures semantic relationships. For example, a query about \"climate impacts\" might retrieve documents mentioning \"environmental effects\" even if the exact words don't match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Who is Joe?\"\n",
        "\n",
        "results = vector_db.similarity_search_with_score(query, k=2)\n",
        "\n",
        "print_document_chunks([results for results, _ in results], context=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up the Chat Model\n",
        "\n",
        "The final stage of RAG integrates retrieval with generation. We construct a combined prompt that includes both the retrieved context and the user's question. The enriched prompt is sent to the LLM model which generates an answer based on both the question and the retrieved context. \n",
        "\n",
        "The prompt explicitly instructs the model to rely on the provided context and admit when it doesn't know, which helps prevent hallucinations (made-up information). The temperature setting (0.7) provides a balance between creative and deterministic responses.\n",
        "\n",
        "This end-to-end pipeline combines the knowledge retrieval capabilities of vector databases with the reasoning and language generation capabilities of LLMs, creating a system that can provide accurate, contextual answers based on specific knowledge sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Set up the chat model, using langchain\n",
        "chat_model = ChatOpenAI(\n",
        "        model_name=\"gpt-4o\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "# Create the RAG prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "    You are a helpful assistant that provides accurate information based on the given context.\n",
        "    If you don't know the answer based on the context, just say that you don't know.\n",
        "    Don't try to make up an answer.\n",
        "    \n",
        "    Context:\n",
        "    {context}\n",
        "    \n",
        "    Question: {question}\n",
        "    \n",
        "    Answer:\n",
        "    \"\"\")\n",
        "\n",
        "# Create a retriever from the vector database\n",
        "k = 3\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "# Format the retrieved documents into a single context string\n",
        "# Also include source numbers for citation\n",
        "def format_docs(docs):\n",
        "    # DEBUG: Print the retrieved documents\n",
        "    print(f\"Retrieved {len(docs)} documents:\")\n",
        "    for i, doc in enumerate(docs):\n",
        "        source_info = f\"Source [{i+1}]\"\n",
        "        print(doc.metadata) \n",
        "        if 'source' in doc.metadata:\n",
        "            source_info = source_info + f\": {doc.metadata['source']}\"\n",
        "        page_content = doc.page_content.replace(\"\\n\", \"\")\n",
        "        print(f\"{source_info}\\n\\t{page_content[:50]} ... {page_content[-50:]}\")\n",
        "\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Create the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt_template\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "question = \"Tell me about Joe\"\n",
        "print(f\"\\nQuestion: {question}\\n\\n\")\n",
        "answer = rag_chain.invoke(question)\n",
        "print(f\"\\n\\nAnswer: {answer}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
