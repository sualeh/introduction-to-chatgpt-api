{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sualeh/introduction-to-chatgpt-api/blob/main/local-vector-database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAaoVeg0yZ7L"
      },
      "source": [
        "----------\n",
        "\n",
        "> **How to Run This Notebook**\n",
        "\n",
        "To get started, create an Open AI API account, set up billing, and generate and API key at https://platform.openai.com/. If you are running the notebook locally in Visual Studio Code or other IDE, create a file called `.env`, and add a line `OPENAI_API_KEY=<your-openai-api-key>`. This key will be read by the `load_dotenv` library.\n",
        "\n",
        "Otherwise, if you are running in Google Colab, create a secret called `OPENAI_API_KEY` and set it to the value of your OpenAI API key.\n",
        "\n",
        "Run the code below to read the key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1OiJxD4yZ7M"
      },
      "outputs": [],
      "source": [
        "%pip install -qq python-dotenv\n",
        "\n",
        "from os import environ as env\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "\n",
        "# Load key from an environmental variable called \"OPENAI_API_KEY\"\n",
        "# Use python-dotenv https://pypi.org/project/python-dotenv/\n",
        "# And take environment variables from .env\n",
        "load_dotenv()\n",
        "try:\n",
        "  # Attempt to read OPENAI_API_KEY from a Google Colab secret\n",
        "  from google.colab import userdata\n",
        "  env['OPENAI_API_KEY'] = env.get('OPENAI_API_KEY', userdata.get('OPENAI_API_KEY'))\n",
        "except ModuleNotFoundError:\n",
        "  print(\"Not running in Google Colab\")\n",
        "  # No action - rely on the OPENAI_API_KEY environmental variable\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vector Databases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define File Loading Functions\n",
        "\n",
        "Define functions to load and process PDF and text files, and test them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qq langchain langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(dotenv_path=\".env.params\")\n",
        "\n",
        "DOCS_DIRECTORY = os.getenv(\"DOCS_DIRECTORY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "def get_files_from_directory(directory_path: str, extensions=['.pdf', '.txt']) -> list[str]:\n",
        "    \"\"\"\n",
        "    Get all files with specified extensions from a directory.\n",
        "    \n",
        "    Args:\n",
        "        directory_path (str): Path to the directory containing files.\n",
        "        extensions (list): List of file extensions to include.\n",
        "    \n",
        "    Returns:\n",
        "        list: List of file paths.\n",
        "    \"\"\"\n",
        "    all_files = []\n",
        "    for ext in extensions:\n",
        "        files = glob.glob(os.path.join(directory_path, f\"*{ext}\"))\n",
        "        all_files.extend(files)\n",
        "    return all_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "def load_document(file_path: str) -> list:\n",
        "    \"\"\"\n",
        "    Load a document based on its file extension.\n",
        "    \n",
        "    Args:\n",
        "        file_path (str): Path to the file.\n",
        "    \n",
        "    Returns:\n",
        "        list: List of document objects.\n",
        "    \"\"\"\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    \n",
        "    if file_extension.lower() == '.pdf':\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        return loader.load()\n",
        "    \n",
        "    elif file_extension.lower() == '.txt':\n",
        "        loader = TextLoader(file_path)\n",
        "        return loader.load()\n",
        "    \n",
        "    else:\n",
        "        logger.error(f\"Unsupported file format: {file_extension}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_documents_from_directory(directory_path):\n",
        "    \"\"\"\n",
        "    Load all documents from a directory.\n",
        "    \n",
        "    Args:\n",
        "        directory_path (str): Path to the directory.\n",
        "    \n",
        "    Returns:\n",
        "        list: List of documents.\n",
        "    \"\"\"\n",
        "    files = get_files_from_directory(directory_path)\n",
        "    \n",
        "    all_documents = []\n",
        "    for file_path in files:\n",
        "        logger.info(f\"Loading {file_path}...\")\n",
        "        documents = load_document(file_path)\n",
        "        all_documents.extend(documents)\n",
        "    \n",
        "    logger.info(f\"Loaded {len(all_documents)} document chunks.\")\n",
        "    return all_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_document_chunks(documents, limit=3):\n",
        "    print()\n",
        "    for index, chunk in enumerate(documents):\n",
        "        if index > limit:\n",
        "            break\n",
        "        print(f\"------ CHUNK {index+1} -------------------------------------------------\")\n",
        "        print(chunk.metadata)\n",
        "        print()\n",
        "        print(chunk.page_content[:100])\n",
        "        print(\"... (skipping content) ...\")\n",
        "        print(chunk.page_content[-100:])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the code for loading files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents = load_documents_from_directory(DOCS_DIRECTORY)\n",
        "\n",
        "print_document_chunks(documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Splitting\n",
        "\n",
        "Next, we'll split the documents into smaller chunks for better embedding and retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_documents(documents, chunk_size=1000, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    Split documents into smaller chunks.\n",
        "    \n",
        "    Args:\n",
        "        documents (list): List of documents.\n",
        "        chunk_size (int): Size of each chunk.\n",
        "        chunk_overlap (int): Overlap between chunks.\n",
        "    \n",
        "    Returns:\n",
        "        list: List of document chunks.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    logger.info(f\"Split into {len(chunks)} chunks.\")\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at the chunks of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks = split_documents(documents)\n",
        "\n",
        "print_document_chunks(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Vector Database\n",
        "\n",
        "Now, let's create functions to build and save our vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qq faiss-cpu faiss-gpu langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "def create_vector_db(chunks, embeddings_model, save_path=None):\n",
        "    \"\"\"\n",
        "    Create a vector database from document chunks.\n",
        "    \n",
        "    Args:\n",
        "        chunks (list): List of document chunks.\n",
        "        embeddings_model: Model to create embeddings.\n",
        "        save_path (str, optional): Path to save the vector database.\n",
        "    \n",
        "    Returns:\n",
        "        FAISS: FAISS vector database.\n",
        "    \"\"\"\n",
        "    vector_db = FAISS.from_documents(chunks, embeddings_model)\n",
        "    \n",
        "    if save_path:\n",
        "        vector_db.save_local(save_path)\n",
        "        logger.info(f\"Vector database saved to {save_path}\")\n",
        "    \n",
        "    return vector_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(dotenv_path=\".env.params\")\n",
        "\n",
        "VECTOR_DB_PATH = os.getenv(\"VECTOR_DB_PATH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "\n",
        "vector_db = create_vector_db(chunks, embeddings_model, save_path=VECTOR_DB_PATH)\n",
        "print(vector_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query Function\n",
        "\n",
        "Let's create a function to query our vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_vector_db(query, vector_db, k=5):\n",
        "    \"\"\"\n",
        "    Query the vector database for similar documents.\n",
        "    \n",
        "    Args:\n",
        "        query (str): Query string.\n",
        "        vector_db: Vector database.\n",
        "        k (int): Number of results to return.\n",
        "    \n",
        "    Returns:\n",
        "        list: List of documents and their similarities.\n",
        "    \"\"\"\n",
        "    results = vector_db.similarity_search_with_score(query, k=k)\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query the vector database to get documents and their similarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What are the main topics covered in 'Who Moved the Cheese'?\"\n",
        "\n",
        "results = query_vector_db(query, vector_db, k=3)\n",
        "\n",
        "print_document_chunks([results for results, _ in results])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
